# -*- coding: utf-8 -*-
"""Proyecto Imagenes Ariel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nh9KCovFoq-ShYTJowaYikJh8mnbyAd9
"""

# Commented out IPython magic to ensure Python compatibility.
#importe de imagenes desde repositorio en github
import sys
if 'google.colab' in sys.modules:
    import subprocess
    subprocess.call("pip install -U opencv-python".split())

import os
import re
from os.path import exists

# %matplotlib inline

project_name = "Proyecto_imagenes"
if not exists(project_name):
  # clone and install
  !git clone -q --recursive https://github.com/Marmotato/semestreprimavera2022
  
  
import sys

import cv2
import numpy as np 
import glob
import matplotlib.pyplot as plt
import math


# Este paquete solo se debe usar si se usa colaboratory
from google.colab.patches import cv2_imshow

"""# 0. Funciones auxiliares"""

# Dibujar imagen con  cubos para mostrar forma 3D
def draw(img, imgpts): #funcion original de cv2
  imgpts = np.int32(imgpts).reshape(-1, 2)
  # draw ground floor in green
  img = cv2.drawContours(img, [imgpts[:4]], -1, (0, 255, 0), -1)
  # draw pillars in blue color
  for i, j in zip(range(4), range(4, 8)):
    img = cv2.line(img, tuple(imgpts[i]), tuple(imgpts[j]), (255), 3)
  # draw top layer in red color
  img = cv2.drawContours(img, [imgpts[4:]], -1, (0, 0, 255), 3)
  return img

# Downsample
def downsample_image(image, reduce_factor):
	for i in range(0,reduce_factor):
		#Imagen a color o grayscale
		if len(image.shape) > 2:
			row,col = image.shape[:2]
		else:
			row,col = image.shape
		image = cv2.pyrDown(image, dstsize= (col//2, row // 2))
	return image

# Crea salida ply para mesh
def create_output(vertices, colors, filename):
	colors = colors.reshape(-1,3)
	vertices = np.hstack([vertices.reshape(-1,3),colors])

	ply_header = '''ply
		format ascii 1.0
		element vertex %(vert_num)d
		property float x
		property float y
		property float z
		property uchar red
		property uchar green
		property uchar blue
		end_header
		'''
	with open(filename, 'w') as f:
		f.write(ply_header %dict(vert_num=len(vertices)))
		np.savetxt(f,vertices,'%f %f %f %d %d %d')

"""# 1. Calibración de camara

Camara usada: Logitech C270
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/semestreprimavera2022/ProyectoImagenes/

square_size = 2.5 #tamaño del cuadrado estimado
img_mask = "./ImgCalibration/*.jpg" #directorio de imagenes de calibración
pattern_size = (9, 6) #cantidad de cuadrados en patrón a analizar
figsize = (20, 20) #tamaño para imagenes

nombre_img = glob.glob(img_mask)
numero_img = len(nombre_img)

pattern_points = np.zeros((np.prod(pattern_size), 3), np.float32) #preparamos para guardar indices
pattern_points[:, :2] = np.indices(pattern_size).T.reshape(-1, 2)
pattern_points *= square_size

obj_points = [] #preparamos para guardar puntos
img_points = []
h, w = cv2.imread(nombre_img[0]).shape[:2]

plt.figure(figsize=figsize)

for i, fn in enumerate(nombre_img):
    print("processing %s... " % fn)

    #transforma imagen cargada a rgb y gray
    imgBGR = cv2.imread(fn)
    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)
    imggray = cv2.cvtColor(imgRGB, cv2.COLOR_RGB2GRAY)

    assert w == imggray.shape[1] and h == imggray.shape[0], f"size: {imggray.shape[1]} x {imggray.shape[0]}" #imagenes deben tener el mismo tamaño
    found, corners = cv2.findChessboardCorners(imggray, pattern_size) #usamos funcion para encontrar esquinas, booleano identifica si estas se encontraron o no

    if not found: #Si no encuentra el patrón, imprimir para ver cuantas calibraciones fueron correctas
        print("patrón no encontrado")
        continue

    if i < 12:
        img_w_corners = cv2.drawChessboardCorners(imgRGB, pattern_size, corners, found)
        plt.subplot(4, 3, i + 1)
        plt.imshow(img_w_corners)
        
    img_points.append(corners.reshape(-1, 2))
    obj_points.append(pattern_points)


plt.show() #mostramos 12 patrones encontrados

# calculate camera distortion
rms, camera_matrix, dist_coefs, _rvecs, _tvecs = cv2.calibrateCamera(obj_points, img_points, (w, h), None, None)

ret, K, dist, rvecs, tvecs = rms, camera_matrix, dist_coefs, _rvecs, _tvecs

#Save parameters into numpy file
np.save("./camera_params/ret", ret)
np.save("./camera_params/K", K)
np.save("./camera_params/dist", dist)
np.save("./camera_params/rvecs", rvecs)
np.save("./camera_params/tvecs", tvecs)

K, dist #coeficientes utilizados para la calibración

#obtenemos focal length

S = cv2.calibrationMatrixValues(camera_matrix, (720,1280), 3.58, 2.02) #valores del sensor de la camara
focal_length = S[2]

# quitar distorsión a partir de la calibración
plt.figure(figsize=figsize)
for i, fn in enumerate(nombre_img): #por cada imagen
    imgBGR = cv2.imread(fn) #leemos imagen y transformamos
    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)
    dst = cv2.undistort(imgRGB, camera_matrix, dist_coefs) #quita distorsión
    if i < 12:
        plt.subplot(4, 3, i + 1)
        plt.imshow(dst)
plt.show()

#plot de cubos 3d a partir del patrón

objectPoints = (
    3
    * square_size
    * np.array([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0], [0, 0, -1], [0, 1, -1], [1, 1, -1], [1, 0, -1]])
) #guardamos puntos para proyección 3d

plt.figure(figsize=figsize)
for i, fn in enumerate(nombre_img):
  try:
    imgBGR = cv2.imread(fn)
    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)
    dst = cv2.undistort(imgRGB, camera_matrix, dist_coefs)
    imgpts = cv2.projectPoints(objectPoints, _rvecs[i], _tvecs[i], camera_matrix, dist_coefs)[0] #proyección de puntos
    drawn_image = draw(dst, imgpts)

    if i < 12: #guardamos hasta 12 imagenes
        plt.subplot(4, 3, i + 1)
        plt.imshow(drawn_image)
  except:
    print("Tuplex index out of range")

plt.show()

"""# 2. Reconstrucción Stereo 3D: Pruebas iniciales

## 2.1 Cara
"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './TestImgs/yoagain1.jpg'
img_path2 = './TestImgs/yoagain2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

"""### 2.1.1 Semi-Global Matching"""

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 3,
 uniquenessRatio = 1,
 speckleWindowSize = 30,
 speckleRange = 20,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'cara1SGBM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""### 2.1.2 Block Matching"""

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso


n_block = 4 # 2 en adelante
n_disp = 2


block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16


# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")

disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)


#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'cara1BM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""## 2.2 Guitarra"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './TestImgs/guitar1.jpg'
img_path2 = './TestImgs/guitar2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

"""### 2.2.1 Semi-Global Matching"""

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 31 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 5,
 speckleWindowSize = 5,
 speckleRange = 5,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'guitar1SGBM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""### 2.2.2 Block Matching"""

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso


n_block = 5 # 2 en adelante
n_disp = 2


block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16


# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")

disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)


#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'guitar1BM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""## 2.3 Silla"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './TestImgs/chairclear1.jpg'
img_path2 = './TestImgs/chairclear2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

"""### 2.3.1 Semi-Global Matching"""

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 7
min_disp = -1
max_disp = 15 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 15,
 speckleWindowSize = 5,
 speckleRange = 5,
 disp12MaxDiff = 5,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'chair1SGBM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""### 2.3.2 Block Matching"""

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso


n_block = 6 # 2 en adelante
n_disp = 1


block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16


# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")

disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)


#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'chair1BM.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""# 3. Train & Test: SGBM

## 3.1 Train
"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/ball1.jpg'
img_path2 = './train/ball2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 6
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 10,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/bottle1.jpg'
img_path2 = './train/bottle2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 7
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 10,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/cup1.jpg'
img_path2 = './train/cup2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 7
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 10,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/dumbell1.jpg'
img_path2 = './train/dumbell2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 7
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/teacup1.jpg'
img_path2 = './train/teacup2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 7
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/trash1.jpg'
img_path2 = './train/trash2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/wood1.jpg'
img_path2 = './train/wood2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

"""## 3.2 Test"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/cuptrash1.jpg'
img_path2 = './test/cuptrash2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'cuptrashsgbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/sneaker1.jpg'
img_path2 = './test/sneaker2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'sneakersgbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/tallgeese1.jpg'
img_path2 = './test/tallgeese2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'tallgeesesgbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/tallgeeseball1.jpg'
img_path2 = './test/tallgeeseball2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
win_size = 5
min_disp = -1
max_disp = 63 #min_disp * 9
num_disp = max_disp - min_disp # Needs to be divisible by 16
# Creacion de objeto block matching. 
stereo = cv2.StereoSGBM_create(minDisparity= min_disp,
 numDisparities = num_disp,
 blockSize = 5,
 uniquenessRatio = 2,
 speckleWindowSize = 20,
 speckleRange = 15,
 disp12MaxDiff = 15,
 P1 = 8*3*win_size**2,#8*3*win_size**2,
 P2 =32*3*win_size**2) #32*3*win_size**2)
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)

#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'tallgeeseballsgbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""# 4. Train & Test: BM

## 4.1 Train
"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/ball1.jpg'
img_path2 = './train/ball2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)


# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso


n_block = 3 # 2 en adelante
n_disp = 4


block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16


# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")

disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)


#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/bottle1.jpg'
img_path2 = './train/bottle2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/cup1.jpg'
img_path2 = './train/cup2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/dumbell1.jpg'
img_path2 = './train/dumbell2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/teacup1.jpg'
img_path2 = './train/teacup2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/trash1.jpg'
img_path2 = './train/trash2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './train/wood1.jpg'
img_path2 = './train/wood2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)
gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 5 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

"""## 4.2 Test"""

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/cuptrash1.jpg'
img_path2 = './test/cuptrash2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 3 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'cuptrashbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/sneaker1.jpg'
img_path2 = './test/sneaker2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 3 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'sneakerbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/tallgeese1.jpg'
img_path2 = './test/tallgeese2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 3 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'tallgeesebm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

#=========================================================
# Stereo 3D reconstruction 
#=========================================================
# Carga de parametros
ret = np.load('./camera_params/ret.npy')
K = np.load('./camera_params/K.npy')
dist = np.load('./camera_params/dist.npy')
# Path de par de imagenes a probar
img_path1 = './test/tallgeeseball1.jpg'
img_path2 = './test/tallgeeseball2.jpg'
# Lectura de imagenes
img_1 = cv2.imread(img_path1)
img_2 = cv2.imread(img_path2)
# Alto y ancho de imagenes 
h,w = img_2.shape[:2]

# Obtención de la mejor matriz para la camara a partir de los parámetros
new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K,dist,(w,h),1,(w,h))
# Remover distorsión de camara
img_1_undistorted = cv2.undistort(img_1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(img_2, K, dist, None, new_camera_matrix)
# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

gray1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)

# Remover distorsión de camara
img_1_undistorted = cv2.undistort(gray1, K, dist, None, new_camera_matrix)
img_2_undistorted = cv2.undistort(gray2, K, dist, None, new_camera_matrix)

# Downsampling de imagenes
img_1_downsampled = downsample_image(img_1_undistorted,3)
img_2_downsampled = downsample_image(img_2_undistorted,3)

# Creando mapa de disparidad
# Parametros de disparidad, ajustados para cada caso
n_block = 3 # 2 en adelante
n_disp = 4

block = 1+(2*n_block) # must be odd, be within 5..255
num_disp = 16*n_disp # Needs to be divisible by 16

# Creacion de objeto block matching. 
stereo = cv2.StereoBM_create(numDisparities=num_disp,
                             blockSize=block) 
#Compute disparity map
print ("\nComputing the disparity  map...")
disparity_map = stereo.compute(img_1_downsampled, img_2_downsampled)
#Show disparity map before generating 3D cloud to verify that point cloud will be usable. 
plt.imshow(disparity_map,'gray')
plt.show()

#Generate  point cloud. 
print ("\nGenerating the 3D map...")
#Get new downsampled width and height 
h,w = img_2_downsampled.shape[:2]
#Load focal length. 
#Perspective transformation matrix
#This transformation matrix is from the openCV documentation, didn't seem to work for me. 
Q = np.float32([[1,0,0,-w/2.0],
    [0,-1,0,h/2.0],
    [0,0,0,-focal_length],
    [0,0,1,0]])
#This transformation matrix is derived from Prof. Didier Stricker's power point presentation on computer vision. 
#Link : https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws14-15/3DCV_lec01_camera.pdf
Q2 = np.float32([[1,0,0,0],
    [0,-1,0,0],
    [0,0,focal_length*0.05,0], #Focal length multiplication obtained experimentally. 
    [0,0,0,1]])
#Reproject points into 3D
points_3D = cv2.reprojectImageTo3D(disparity_map, Q2)
#Get color points
colors = cv2.cvtColor(img_1_downsampled, cv2.COLOR_BGR2RGB)
#Get rid of points with value 0 (i.e no depth)
mask_map = disparity_map > disparity_map.min()
#Mask colors and points. 
output_points = points_3D[mask_map]
output_colors = colors[mask_map]
#Define name for output file
output_file = 'tallgeeseballbm.ply'
#Generate point cloud 
print ("\n Creating the output file... \n")
create_output(output_points, output_colors, output_file)

"""# 5. Deep Learning: MeshRCNN """

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/semestreprimavera2022/ProyectoImagenes/

!git clone -q --recursive https://github.com/facebookresearch/detectron2

"""## Detectron 2"""

!python -m pip install pyyaml==5.1
import sys, os, distutils.core

dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
sys.path.insert(0, os.path.abspath('./detectron2'))

# Properly install detectron2. (Please do not install twice in both ways)
# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
im = cv2.imread("./input.jpg")
cv2_imshow(im)

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

im = cv2.imread("/content/semestreprimavera2022/ProyectoImagenes/TestImgs/chair2.jpg")
cv2_imshow(im)

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/semestreprimavera2022/ProyectoImagenes/

"""## Instalacion completa"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/semestreprimavera2022/ProyectoImagenes/

# Commented out IPython magic to ensure Python compatibility.
# install dependencies: (use cu101 because colab has CUDA 10.1)
# %env FORCE_CUDA=1
!pip install -U torch==1.6 torchvision==0.7 -f https://download.pytorch.org/whl/cu101/torch_stable.html 
!pip install pyyaml==5.1 pycocotools>=2.0.1
!pip install -U fvcore==0.1.3.post20210317
!pip install -U --force-reinstall iopath==0.1.6 
!pip install 'git+https://github.com/facebookresearch/detectron2.git@4b539e41f4b19570bc6b722fa0f516f927cdae02'
!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'

import torch, torchvision
!git clone https://github.com/facebookresearch/meshrcnn.git
!cd meshrcnn && pip install -e .

# pix3d data
!./meshrcnn/datasets/pix3d/download_pix3d.sh

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/semestreprimavera2022/ProyectoImagenes

"""## Demo : Obtención de mascara"""

from google.colab import files
uploaded = files.upload()  # importar imagen como test.jpg

!python ./meshrcnn/demo/demo.py --config-file ./meshrcnn/configs/pix3d/meshrcnn_R50_FPN.yaml \
--input ./test.jpg --output output_demo --onlyhighest MODEL.WEIGHTS meshrcnn://meshrcnn_R50.pth

"""## Visualización"""

filename = 'test.jpg' #nombre como test.jpg o del archivo subido
maskname = 'chair_0.994' # Cambiar nombre al de la mascara en 'output_demo/test/0_mask_sofa_1.000.png'

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))

img = plt.imread(filename) 
plt.imshow(img)
plt.axis('off')
plt.show()

plt.figure(figsize=(10,10))
img = plt.imread("./output_demo/test/0_mask_" + maskname+'.png')  
plt.imshow(img)
plt.axis('off')
plt.show()

"""## Descarga del Mesh resultante"""

from google.colab import files
files.download('/content/semestreprimavera2022/ProyectoImagenes/output_demo/test/0_mesh_'+ maskname + '.obj')